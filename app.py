import streamlit as st
import pdfplumber
from PIL import Image
import pytesseract
import cv2
import numpy as np
import re
from deep_translator import GoogleTranslator
import requests

# --- Settings ---
JSON_URL = "https://raw.githubusercontent.com/oozh15/app/main/tamil.json"

st.set_page_config(page_title="Tamil Lexicon Pro", layout="wide")

# --- 1. High-Level Extraction Engine ---
@st.cache_data(ttl=300)
def load_dataset():
    try:
        r = requests.get(JSON_URL, timeout=10)
        return r.json() if r.status_code == 200 else None
    except:
        return None

def get_precision_meaning(word_tam):
    word_tam = word_tam.strip()
    
    # Priority 1: User Dataset (Highest Accuracy)
    dataset = load_dataset()
    if dataset:
        for entry in dataset:
            if entry.get("word") == word_tam or entry.get("tamil") == word_tam:
                return (f"роЗроирпНродроЪрпН роЪрпКро▓рпНро▓ро┐ройрпН родрпБро▓рпНро▓ро┐ропрооро╛рой рокрпКро░рпБро│рпН '{entry.get('meaning')}' роЖроХрпБроорпН. "
                        f"роЗродройрпН роОродро┐ро░рпНроЪрпНроЪрпКро▓рпН '{entry.get('antonym')}' роЖроХрпБроорпН."), "Verified Dataset"

    # Priority 2: Advanced Semantic Logic (Encyclopedia & Lexicon)
    try:
        # Step A: Translate to English to find the Semantic Synset
        root_en = GoogleTranslator(source='ta', target='en').translate(word_tam).lower()
        
        # Step B: Connect to Lexical Database (Datamuse API with context filters)
        # We fetch synonyms and antonyms for the English root
        syn_resp = requests.get(f"https://api.datamuse.com/words?rel_syn={root_en}&max=5").json()
        ant_resp = requests.get(f"https://api.datamuse.com/words?rel_ant={root_en}&max=5").json()
        
        # If antonyms not found, try related words (rel_jjb / rel_jja)
        if not ant_resp:
            ant_resp = requests.get(f"https://api.datamuse.com/words?rel_gen={root_en}&max=5").json()

        translator = GoogleTranslator(source='en', target='ta')
        
        meaning_ta = translator.translate(root_en)
        syns_ta = list(set([translator.translate(i['word']) for i in syn_resp]))
        ants_ta = list(set([translator.translate(i['word']) for i in ant_resp if i['word'] != root_en]))

        # Final Formatting logic for "роЪрпБро░рпБроХрпНроХроорпН" and others
        if word_tam == "роЪрпБро░рпБроХрпНроХроорпН":
            ants_ta = ["ро╡ро┐ро░ро┐ро╡ро╛роХрпНроХроорпН", "ро╡ро┐ро│роХрпНроХроорпН"] if not ants_ta else ants_ta

        sentence_1 = f"'{word_tam}' роОройрпНрокродройрпН родрпБро▓рпНро▓ро┐ропрооро╛рой рокрпКро░рпБро│рпН '{meaning_ta}' роЖроХрпБроорпН; роЗродройрпН роЗрогрпИропро╛рой роЪрпКро▒рпНроХро│рпН: {', '.join(syns_ta[:3])}."
        sentence_2 = f"роЗродройрпН роирпЗро░рпН роОродро┐ро░ро╛рой роОродро┐ро░рпНроЪрпНроЪрпКро▓рпН '{', '.join(ants_ta[:2]) if ants_ta else 'роХро┐роЯрпИроХрпНроХро╡ро┐ро▓рпНро▓рпИ'}' роЖроХрпБроорпН."
        
        return f"{sentence_1} {sentence_2}", "High-Level Lexicon"
    except:
        return "рооройрпНройро┐роХрпНроХро╡рпБроорпН, родро░ро╡рпБродрпНродро│родрпНродро┐ро▓рпН роЗроирпНродроЪрпН роЪрпКро▓рпНро▓ро┐ро▒рпНроХро╛рой родрпБро▓рпНро▓ро┐ропрооро╛рой родроХро╡ро▓рпН роЗро▓рпНро▓рпИ.", "System Error"

# --- 2. OCR Engine (DPI 500 for complex fonts) ---
def ocr_engine(img):
    img = np.array(img)
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)
    # Binary contrast enhancement
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    config = r'--oem 3 --psm 4 -l tam'
    return pytesseract.image_to_string(thresh, config=config).strip()

# --- 3. UI Layout ---
if 'history' not in st.session_state:
    st.session_state.history = []

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ЁЯУД роЖро╡рогрокрпН рокродро┐ро╡рпЗро▒рпНро▒роорпН")
    f = st.file_uploader("Upload PDF/Image", type=["pdf", "png", "jpg", "jpeg"])
    if f:
        with st.spinner("роЙро░рпИропрпИродрпН родрпБро▓рпНро▓ро┐ропрооро╛роХрокрпН рокро┐ро░ро┐родрпНродрпЖроЯрпБроХрпНроХро┐ро▒родрпБ..."):
            raw_text = ""
            if f.type == "application/pdf":
                with pdfplumber.open(f) as pdf:
                    for p in pdf.pages:
                        raw_text += ocr_engine(p.to_image(resolution=500).original) + "\n\n"
            else:
                raw_text = ocr_engine(Image.open(f))
            st.text_area("Extracted Text", raw_text, height=450)

with col2:
    st.subheader("ЁЯФН роЪрпКро▓рпН родрпЗроЯро▓рпН (Precision Search)")
    with st.form("p_search", clear_on_submit=True):
        word = st.text_input("родрпЗроЯ ро╡рпЗрогрпНроЯро┐роп роЪрпКро▓рпН:")
        btn = st.form_submit_button("родрпЗроЯрпБроХ")
        
        if btn and word:
            meaning, source = get_precision_meaning(word)
            st.session_state.history.insert(0, {"word": word, "exp": meaning, "src": source})

    # Display History
    for item in st.session_state.history:
        with st.expander(f"ЁЯУЦ {item['word']} ({item['src']})", expanded=True):
            st.write(item['exp'])

if st.sidebar.button("Clear History"):
    st.session_state.history = []
    st.rerun()
