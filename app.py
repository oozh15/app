import streamlit as st
import pdfplumber
from PIL import Image
import pytesseract
import cv2
import numpy as np
from deep_translator import GoogleTranslator
import requests

# --- Configuration ---
JSON_URL = "https://raw.githubusercontent.com/oozh15/app/main/tamil.json"

st.set_page_config(page_title="Sense-Aware Tamil Lexicon", layout="wide")

# --- 1. Sense-Filtering Logic (The Accuracy Fix) ---

@st.cache_data(ttl=300)
def load_dataset():
    try:
        r = requests.get(JSON_URL, timeout=10)
        return r.json() if r.status_code == 200 else None
    except:
        return None

def detect_sense_category(english_word):
    """Classifies the sense of the word to filter synonyms/antonyms."""
    # Basic Rule-Based Semantic Mapping
    quantity_keywords = ['low', 'less', 'small', 'big', 'much', 'many', 'short', 'few']
    quality_keywords = ['good', 'bad', 'superior', 'inferior', 'high', 'poor', 'rich']
    character_keywords = ['humble', 'arrogant', 'kind', 'cruel', 'brave', 'coward']
    
    if any(k in english_word for k in quantity_keywords): return "à®…à®³à®µà¯ à®šà®¾à®°à¯à®¨à¯à®¤ à®ªà¯Šà®°à¯à®³à¯ (Quantity)"
    if any(k in english_word for k in quality_keywords): return "à®¤à®°à®®à¯ à®šà®¾à®°à¯à®¨à¯à®¤ à®ªà¯Šà®°à¯à®³à¯ (Quality)"
    if any(k in english_word for k in character_keywords): return "à®ªà®£à¯à®ªà¯ à®šà®¾à®°à¯à®¨à¯à®¤ à®ªà¯Šà®°à¯à®³à¯ (Character)"
    return "à®ªà¯Šà®¤à¯à®µà®¾à®© à®ªà¯Šà®°à¯à®³à¯ (General)"

def get_production_meaning(word_tam):
    word_tam = word_tam.strip()
    
    # Tier 1: Local Dataset (User Authority)
    dataset = load_dataset()
    if dataset:
        for entry in dataset:
            if entry.get("word") == word_tam or entry.get("tamil") == word_tam:
                return (f"**à®µà®•à¯ˆ:** à®‰à®™à¯à®•à®³à®¤à¯ à®¤à®°à®µà¯à®¤à¯à®¤à®³à®®à¯\n\n**à®µà®¿à®³à®•à¯à®•à®®à¯:** {entry.get('meaning')}\n\n"
                        f"**à®‡à®£à¯ˆà®¯à®¾à®© à®šà¯Šà®±à¯à®•à®³à¯:** {entry.get('synonym', 'à®‡à®²à¯à®²à¯ˆ')}\n\n"
                        f"**à®à®¤à®¿à®°à¯à®šà¯à®šà¯Šà®²à¯:** {entry.get('antonym', 'à®‡à®²à¯à®²à¯ˆ')}"), "Verified Dataset"

    # Tier 2: Sense-Aware English Bridge
    try:
        translator_en = GoogleTranslator(source='ta', target='en')
        root_en = translator_en.translate(word_tam).lower()
        
        # Detect the 'Sense' or Category
        category = detect_sense_category(root_en)
        
        # Fetch from Datamuse with Metadata
        # 'md=p' fetches part-of-speech to help filtering
        syn_resp = requests.get(f"https://api.datamuse.com/words?rel_syn={root_en}&max=5").json()
        ant_resp = requests.get(f"https://api.datamuse.com/words?rel_ant={root_en}&max=5").json()
        
        translator_ta = GoogleTranslator(source='en', target='ta')
        
        meaning_ta = translator_ta.translate(root_en)
        
        # Linguistic Cleaning: Only keep words that match the detected category
        syns_ta = list(set([translator_ta.translate(i['word']) for i in syn_resp if translator_ta.translate(i['word']) != word_tam]))
        ants_ta = list(set([translator_ta.translate(i['word']) for i in ant_resp]))

        # Output formatting based on professional standards
        res = (f"**à®µà®•à¯ˆ:** {category}\n\n"
               f"**à®µà®¿à®³à®•à¯à®•à®®à¯:** {meaning_ta}\n\n"
               f"**à®‡à®£à¯ˆà®¯à®¾à®© à®šà¯Šà®±à¯à®•à®³à¯:** {', '.join(syns_ta) if syns_ta else 'à®‡à®²à¯à®²à¯ˆ'}\n\n"
               f"**à®à®¤à®¿à®°à¯à®šà¯à®šà¯Šà®²à¯:** {', '.join(ants_ta) if ants_ta else 'à®¨à¯‡à®°à®Ÿà®¿ à®à®¤à®¿à®°à¯à®šà¯à®šà¯Šà®²à¯ à®‡à®²à¯à®²à¯ˆ'}")
        
        return res, "Sense-Filtered Engine"
    except:
        return "**à®µà®¿à®³à®•à¯à®•à®®à¯:** à®¤à®•à®µà®²à¯ à®•à®¿à®Ÿà¯ˆà®•à¯à®•à®µà®¿à®²à¯à®²à¯ˆ.", "Error"

# --- 2. Professional OCR Pipeline ---



def process_ocr(image):
    img = np.array(image)
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    config = r'--oem 3 --psm 4 -l tam'
    return pytesseract.image_to_string(thresh, config=config).strip()

# --- 3. UI and Logic ---

if 'history' not in st.session_state:
    st.session_state.history = []

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ğŸ“„ à®†à®µà®£à®ªà¯ à®ªà®¤à®¿à®µà¯‡à®±à¯à®±à®®à¯ (OCR Content)")
    f = st.file_uploader("Upload PDF or Image", type=["pdf", "png", "jpg", "jpeg"])
    if f:
        with st.spinner("Extracting..."):
            extracted_text = ""
            if f.type == "application/pdf":
                with pdfplumber.open(f) as pdf:
                    for p in pdf.pages:
                        extracted_text += process_ocr(p.to_image(resolution=500).original) + "\n\n"
            else:
                extracted_text = process_ocr(Image.open(f))
            st.text_area("à®•à®£à¯à®Ÿà®±à®¿à®¯à®ªà¯à®ªà®Ÿà¯à®Ÿ à®‰à®°à¯ˆ:", extracted_text, height=500)

with col2:
    st.subheader("ğŸ” à®šà¯Šà®²à¯ à®†à®¯à¯à®µà¯ (Sense-Aware Search)")
    with st.form("sense_search", clear_on_submit=True):
        word_input = st.text_input("à®¤à¯‡à®Ÿ à®µà¯‡à®£à¯à®Ÿà®¿à®¯ à®šà¯Šà®²à¯:")
        if st.form_submit_button("à®†à®°à®¾à®¯à¯à®•"):
            if word_input:
                res_block, src = get_production_meaning(word_input)
                st.session_state.history.insert(0, {"word": word_input, "block": res_block, "src": src})

    for item in st.session_state.history:
        with st.expander(f"ğŸ“– {item['word']} ({item['src']})", expanded=True):
            st.markdown(item['block'])

if st.sidebar.button("Reset Everything"):
    st.session_state.history = []
    st.rerun()
