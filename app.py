import streamlit as st
import pdfplumber
from PIL import Image
import pytesseract
import cv2
import numpy as np
from deep_translator import GoogleTranslator
import requests

# --- 1. родро░ро╡рпБродрпНродро│роорпН рооро▒рпНро▒рпБроорпН ро╡ро┐родро┐роХро│рпН ---
JSON_URL = "https://raw.githubusercontent.com/oozh15/app/main/tamil.json"

# AI роорпБро▒рпИроХрпНроХро╛рой роОродро┐ро░рпНроЪрпНроЪрпКро▓рпН ро╡ро┐родро┐роХро│рпН
ANTONYM_RULES = {
    "emotions": ["роЕро▓роЯрпНроЪро┐ропроорпН", "ро╡рпЖро▒рпБрокрпНрокрпБ", "роХро╡ро▓рпИропро┐ройрпНроорпИ"],
    "quantity": ["роЕродро┐роХ", "роХрпВроЯрпБродро▓рпН", "роорпЗро▓рпН"],
    "quality": ["роЪро┐ро▒роирпНрод", "роЙропро░рпНроирпНрод", "роорпЗроорпНрокроЯрпНроЯ"],
    "general": ["роОродро┐ро░рпНроЪрпНроЪрпКро▓рпН роХро┐роЯрпИроХрпНроХро╡ро┐ро▓рпНро▓рпИ"]
}

st.set_page_config(page_title="Dataset-First Lexicon", layout="wide")

# --- 2. родрпБро▓рпНро▓ро┐ропрооро╛рой родрпЗроЯро▓рпН роЪрпЖропро▓рпНрокро╛роЯрпБроХро│рпН ---

@st.cache_data(ttl=60)
def load_verified_data():
    """роЙроЩрпНроХро│родрпБ GitHub JSON родро░ро╡рпИ рокродро┐ро╡ро┐ро▒роХрпНроХроорпН роЪрпЖропрпНропрпБроорпН."""
    try:
        r = requests.get(JSON_URL, timeout=5)
        return r.json() if r.status_code == 200 else None
    except:
        return None

def get_word_analysis(word_tam, ocr_context=""):
    word_tam = word_tam.strip() # родрпЗроЯрпБроорпН роЪрпКро▓рпНро▓ро┐ро▓рпН роЙро│рпНро│ родрпЗро╡рпИропро┐ро▓рпНро▓ро╛род ро╕рпНрокрпЗро╕рпИ роирпАроХрпНроХрпБродро▓рпН
    
    # --- LEVEL 1: DATASET (роорпБродро▓рпН роорпБройрпНройрпБро░ро┐роорпИ) ---
    dataset = load_verified_data()
    if dataset:
        for entry in dataset:
            # родро░ро╡рпБродрпНродро│родрпНродро┐ро▓рпН роЙро│рпНро│ роЪрпКро▓рпНро▓рпБроЯройрпН родрпБро▓рпНро▓ро┐ропрооро╛роХ роТрокрпНрокро┐роЯрпБродро▓рпН
            db_word = str(entry.get("word", entry.get("tamil", ""))).strip()
            if db_word == word_tam:
                return (f"**роЖродро╛ро░роорпН:** роЙроЩрпНроХро│родрпБ родро░ро╡рпБродрпНродро│роорпН (Verified Dataset)\n\n"
                        f"**ро╡ро┐ро│роХрпНроХроорпН:** {entry.get('meaning')}\n\n"
                        f"**роЗрогрпИропро╛рой роЪрпКро▒рпНроХро│рпН:** {entry.get('synonym', 'роЗро▓рпНро▓рпИ')}\n\n"
                        f"**роОродро┐ро░рпНроЪрпНроЪрпКро▓рпН:** {entry.get('antonym', 'роЗро▓рпНро▓рпИ')}"), "Dataset Match"

    # --- LEVEL 2: SENSE-AWARE AI (родро░ро╡рпБродрпНродро│родрпНродро┐ро▓рпН роЗро▓рпНро▓рпИ роОройро┐ро▓рпН роороЯрпНроЯрпБроорпН) ---
    try:
        to_en = GoogleTranslator(source='ta', target='en')
        to_ta = GoogleTranslator(source='en', target='ta')
        
        # роЪрпВро┤ро▓рпИ роХрогрпНроЯро▒ро┐родро▓рпН (Context)
        context_sentence = word_tam
        if ocr_context:
            for line in ocr_context.splitlines():
                if word_tam in line:
                    context_sentence = line.strip()
                    break
        
        en_word = to_en.translate(word_tam).lower()
        en_context = to_en.translate(context_sentence).lower()

        # родрпБро▓рпНро▓ро┐ропродрпНродройрпНроорпИ родро┐ро░рпБродрпНродроорпН: Care/Concern рокро┐ро┤рпИроХро│рпИродрпН родро╡ро┐ро░рпНродрпНродро▓рпН
        sense = "general"
        if any(k in en_word or k in en_context for k in ["care", "concern", "worry"]):
            sense = "emotions"
            meaning_ta = "роХро╡ройро┐рокрпНрокрпБ / роЖро░рпНро╡роорпН"
            syns_ta = ["роХро╡ройроорпН", "роИроЯрпБрокро╛роЯрпБ", "рокро▒рпНро▒рпБ"]
        else:
            meaning_ta = to_ta.translate(en_word)
            # API роорпВро▓роорпН роЗрогрпИропро╛рой роЪрпКро▒рпНроХро│рпИрокрпН рокрпЖро▒рпБродро▓рпН
            syn_resp = requests.get(f"https://api.datamuse.com/words?rel_syn={en_word}&max=3").json()
            syns_ta = [to_ta.translate(i['word']) for i in syn_resp]

        ants_ta = ANTONYM_RULES.get(sense, ["роХро┐роЯрпИроХрпНроХро╡ро┐ро▓рпНро▓рпИ"])

        res = (f"**роЖродро╛ро░роорпН:** роЪрпЖропро▒рпНроХрпИ роирпБрогрпНрогро▒ро┐ро╡рпБ (Lexical Bridge)\n\n"
               f"**ро╡ро┐ро│роХрпНроХроорпН:** {meaning_ta}\n\n"
               f"**роЗрогрпИропро╛рой роЪрпКро▒рпНроХро│рпН:** {', '.join(syns_ta) if syns_ta else 'роЗро▓рпНро▓рпИ'}\n\n"
               f"**роОродро┐ро░рпНроЪрпНроЪрпКро▓рпН:** {', '.join(ants_ta)}")
        
        return res, "AI Bridge"
    except:
        return "рооройрпНройро┐роХрпНроХро╡рпБроорпН, родроХро╡ро▓рпН роХро┐роЯрпИроХрпНроХро╡ро┐ро▓рпНро▓рпИ.", "Error"

# --- 3. роорпЗроорпНрокроЯрпБродрпНродрокрпНрокроЯрпНроЯ OCR ---

def process_ocr(image):
    img = np.array(image)
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    # родрпЖро│ро┐ро╡рпБроХрпНроХро╛роХ 2 роороЯроЩрпНроХрпБ рокрпЖро░ро┐родро╛роХрпНроХрпБродро▓рпН
    gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)
    # роЗро░рпИроЪрпНроЪро▓рпИ роирпАроХрпНроХрпБродро▓рпН (Denoising)
    denoised = cv2.fastNlMeansDenoising(gray, h=10)
    _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    config = r'--oem 3 --psm 4 -l tam'
    return pytesseract.image_to_string(thresh, config=config).strip()

# --- 4. рокропройро░рпН роЗроЯрпИроорпБроХроорпН (UI) ---

if 'history' not in st.session_state:
    st.session_state.history = []

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ЁЯУД роЖро╡рогрокрпН рокродро┐ро╡рпЗро▒рпНро▒роорпН")
    f = st.file_uploader("Upload", type=["pdf", "png", "jpg", "jpeg"])
    ocr_text = ""
    if f:
        with st.spinner("OCR родро░ро╡рпИрокрпН рокро┐ро░ро┐роХрпНроХро┐ро▒родрпБ..."):
            if f.type == "application/pdf":
                with pdfplumber.open(f) as pdf:
                    for p in pdf.pages:
                        ocr_text += process_ocr(p.to_image(resolution=500).original) + "\n\n"
            else:
                ocr_text = process_ocr(Image.open(f))
            st.text_area("роХрогрпНроЯро▒ро┐ропрокрпНрокроЯрпНроЯ роЙро░рпИ:", ocr_text, height=450)

with col2:
    st.subheader("ЁЯФН родрпБро▓рпНро▓ро┐ропрооро╛рой роЖропрпНро╡рпБ")
    with st.form("search_form", clear_on_submit=True):
        word_input = st.text_input("родрпЗроЯ ро╡рпЗрогрпНроЯро┐роп роЪрпКро▓рпН:")
        if st.form_submit_button("роЖро░ро╛ропрпНроХ"):
            if word_input:
                res_block, src = get_word_analysis(word_input, ocr_text)
                st.session_state.history.insert(0, {"word": word_input, "block": res_block, "src": src})

    for item in st.session_state.history:
        with st.expander(f"ЁЯУЦ {item.get('word')} ({item.get('src')})", expanded=True):
            st.markdown(item.get('block'))

if st.sidebar.button("ЁЯЧСя╕П Reset"):
    st.session_state.history = []
    st.rerun()
