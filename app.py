import streamlit as st
import pdfplumber
from PIL import Image
import pytesseract
import cv2
import numpy as np
from deep_translator import GoogleTranslator
import requests

# --- 1. Accuracy Rules & Assets ---
JSON_URL = "https://raw.githubusercontent.com/oozh15/app/main/tamil.json"

# Semantic Rules for Accuracy when Dataset fails
ANTONYM_RULES = {
    "quantity": ["роЕродро┐роХ", "роХрпВроЯрпБродро▓рпН", "роорпЗро▓рпН"],
    "height": ["роЙропро░рооро╛рой", "роирпЖроЯро┐роп"],
    "quality": ["роЪро┐ро▒роирпНрод", "роЙропро░рпНроирпНрод", "роорпЗроорпНрокроЯрпНроЯ"],
    "character": ["роЕроХроирпНродрпИропрпБро│рпНро│", "роЪрпЖро░рпБроХрпНроХрпБроЯрпИроп"],
    "general": ["роОродро┐ро░рпНроЪрпНроЪрпКро▓рпН роХро┐роЯрпИроХрпНроХро╡ро┐ро▓рпНро▓рпИ"]
}

st.set_page_config(page_title="Priority Tamil Lexicon", layout="wide")

# --- 2. Accuracy Engine ---

@st.cache_data(ttl=300)
def load_dataset():
    """Fetches your GitHub dataset as the primary source of truth."""
    try:
        r = requests.get(JSON_URL, timeout=10)
        return r.json() if r.status_code == 200 else None
    except:
        return None

def detect_sense(en_text):
    """Categorizes the word sense to pick correct antonyms."""
    text = en_text.lower()
    if any(k in text for k in ["amount", "less", "reduced", "quantity", "low"]): return "quantity"
    if any(k in text for k in ["short", "height", "tall"]): return "height"
    if any(k in text for k in ["quality", "standard", "inferior"]): return "quality"
    if any(k in text for k in ["humble", "modest", "arrogant"]): return "character"
    return "general"

def get_accurate_meaning(word_tam, ocr_context=""):
    word_tam = word_tam.strip()
    
    # --- PRIORITY 1: DATASET LINK ---
    dataset = load_dataset()
    if dataset:
        for entry in dataset:
            # Check for word match in your JSON
            if entry.get("word") == word_tam or entry.get("tamil") == word_tam:
                return (f"**роЖродро╛ро░роорпН:** роЙроЩрпНроХро│родрпБ родро░ро╡рпБродрпНродро│роорпН (Dataset)\n\n"
                        f"**ро╡ро┐ро│роХрпНроХроорпН:** {entry.get('meaning')}\n\n"
                        f"**роЗрогрпИропро╛рой роЪрпКро▒рпНроХро│рпН:** {entry.get('synonym', 'роЗро▓рпНро▓рпИ')}\n\n"
                        f"**роОродро┐ро░рпНроЪрпНроЪрпКро▓рпН:** {entry.get('antonym', 'роЗро▓рпНро▓рпИ')}"), "Verified Dataset"

    # --- PRIORITY 2: CONTEXTUAL BRIDGE (Other Ideas) ---
    try:
        to_en = GoogleTranslator(source='ta', target='en')
        to_ta = GoogleTranslator(source='en', target='ta')
        
        # Look for the sentence containing the word in OCR text for context
        sentence = word_tam
        if ocr_context:
            for line in ocr_context.splitlines():
                if word_tam in line:
                    sentence = line.strip()
                    break
        
        en_sentence = to_en.translate(sentence)
        en_word = to_en.translate(word_tam).lower()
        
        # Detect Sense & Fetch Synonyms
        sense = detect_sense(en_sentence)
        syn_resp = requests.get(f"https://api.datamuse.com/words?rel_syn={en_word}&max=3").json()
        
        # Rule-based filtering
        syns_ta = [to_ta.translate(i['word']) for i in syn_resp]
        ants_ta = ANTONYM_RULES.get(sense, ["роХро┐роЯрпИроХрпНроХро╡ро┐ро▓рпНро▓рпИ"])

        res = (f"**роЖродро╛ро░роорпН:** роЪрпЖропро▒рпНроХрпИ роирпБрогрпНрогро▒ро┐ро╡рпБ (Sense-Aware)\n\n"
               f"**ро╡ро┐ро│роХрпНроХроорпН:** {to_ta.translate(en_word)}\n\n"
               f"**роЪрпВро┤ро▓рпН:** {sentence}\n\n"
               f"**роЗрогрпИропро╛рой роЪрпКро▒рпНроХро│рпН:** {', '.join(syns_ta) if syns_ta else 'роЗро▓рпНро▓рпИ'}\n\n"
               f"**роОродро┐ро░рпНроЪрпНроЪрпКро▓рпН:** {', '.join(ants_ta)}")
        
        return res, f"Sense: {sense.capitalize()}"
    except:
        return "**ро╡ро┐ро│роХрпНроХроорпН:** родроХро╡ро▓рпН роХро┐роЯрпИроХрпНроХро╡ро┐ро▓рпНро▓рпИ.", "Error"

# --- 3. Professional OCR ---



def process_ocr(image):
    img = np.array(image)
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    gray = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    config = r'--oem 3 --psm 4 -l tam'
    return pytesseract.image_to_string(thresh, config=config).strip()

# --- 4. UI Layout ---

if 'history' not in st.session_state:
    st.session_state.history = []

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ЁЯУД роЖро╡рогрокрпН рокродро┐ро╡рпЗро▒рпНро▒роорпН")
    f = st.file_uploader("Upload", type=["pdf", "png", "jpg", "jpeg"])
    ocr_text = ""
    if f:
        if f.type == "application/pdf":
            with pdfplumber.open(f) as pdf:
                for p in pdf.pages:
                    ocr_text += process_ocr(p.to_image(resolution=500).original) + "\n\n"
        else:
            ocr_text = process_ocr(Image.open(f))
        st.text_area("Extracted Text", ocr_text, height=450, key="main_ocr")

with col2:
    st.subheader("ЁЯФН роЙропро░рпН-родрпБро▓рпНро▓ро┐роп роЖропрпНро╡рпБ")
    with st.form("acc_search", clear_on_submit=True):
        word_input = st.text_input("родрпЗроЯ ро╡рпЗрогрпНроЯро┐роп роЪрпКро▓рпН:")
        if st.form_submit_button("роЖро░ро╛ропрпНроХ"):
            if word_input:
                res_block, src = get_accurate_meaning(word_input, ocr_text)
                st.session_state.history.insert(0, {"word": word_input, "block": res_block, "src": src})

    for item in st.session_state.history:
        # Crash-proof history rendering
        w = item.get('word', 'Unknown')
        b = item.get('block', 'No data')
        s = item.get('src', 'Sense')
        with st.expander(f"ЁЯУЦ {w} ({s})", expanded=True):
            st.markdown(b)

if st.sidebar.button("ЁЯЧСя╕П Reset History"):
    st.session_state.history = []
    st.rerun()
